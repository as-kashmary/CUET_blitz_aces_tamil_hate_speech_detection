{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:00.562160Z",
     "iopub.status.busy": "2025-03-30T05:32:00.561889Z",
     "iopub.status.idle": "2025-03-30T05:32:00.866355Z",
     "shell.execute_reply": "2025-03-30T05:32:00.865665Z",
     "shell.execute_reply.started": "2025-03-30T05:32:00.562140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sharedtaskhatespeechincaste/dev.csv\n",
      "/kaggle/input/sharedtaskhatespeechincaste/train (1).csv\n",
      "/kaggle/input/sharedtaskhatespeechincaste/test (1).csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:00.867857Z",
     "iopub.status.busy": "2025-03-30T05:32:00.867433Z",
     "iopub.status.idle": "2025-03-30T05:32:11.706212Z",
     "shell.execute_reply": "2025-03-30T05:32:11.705565Z",
     "shell.execute_reply.started": "2025-03-30T05:32:00.867833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import AdamW, AutoTokenizer\n",
    "from sklearn.utils import class_weight\n",
    "import itertools\n",
    "from scipy.stats import mode\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.708102Z",
     "iopub.status.busy": "2025-03-30T05:32:11.707743Z",
     "iopub.status.idle": "2025-03-30T05:32:11.804386Z",
     "shell.execute_reply": "2025-03-30T05:32:11.803544Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.708080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tamil_train = pd.read_csv('/kaggle/input/sharedtaskhatespeechincaste/train (1).csv')  # Replace with the correct path to your file\n",
    "tamil_dev = pd.read_csv('/kaggle/input/sharedtaskhatespeechincaste/dev.csv')\n",
    "tamil_test = pd.read_csv('/kaggle/input/sharedtaskhatespeechincaste/test (1).csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.805776Z",
     "iopub.status.busy": "2025-03-30T05:32:11.805558Z",
     "iopub.status.idle": "2025-03-30T05:32:11.811104Z",
     "shell.execute_reply": "2025-03-30T05:32:11.810371Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.805758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1576, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.812119Z",
     "iopub.status.busy": "2025-03-30T05:32:11.811901Z",
     "iopub.status.idle": "2025-03-30T05:32:11.848006Z",
     "shell.execute_reply": "2025-03-30T05:32:11.847016Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.812101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4594</td>\n",
       "      <td>Humanity paatha intha slag countries prblm var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3593</td>\n",
       "      <td>Avanga Holi kondada oruku poranga da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2533</td>\n",
       "      <td>Avangala avanga ooruke anupura vazhiya parunga.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6975</td>\n",
       "      <td>பைத்தியமாடா நீ.. நான் சோழிய வேளாளர். நான் என் ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6011</td>\n",
       "      <td>Caste is there  parayan think to marry higheru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  4594  Humanity paatha intha slag countries prblm var...\n",
       "1  3593               Avanga Holi kondada oruku poranga da\n",
       "2  2533    Avangala avanga ooruke anupura vazhiya parunga.\n",
       "3  6975  பைத்தியமாடா நீ.. நான் சோழிய வேளாளர். நான் என் ...\n",
       "4  6011  Caste is there  parayan think to marry higheru..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.849213Z",
     "iopub.status.busy": "2025-03-30T05:32:11.848935Z",
     "iopub.status.idle": "2025-03-30T05:32:11.863273Z",
     "shell.execute_reply": "2025-03-30T05:32:11.862684Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.849188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select the second and third columns (columns with index 1 and 2)\n",
    "tamil_train = tamil_train.iloc[:, 1:3]  # Select columns from index 1 to 2\n",
    "tamil_train = tamil_train.rename(columns={1: \"text\", 2: \"label\"})  # Rename columns to 'text' and 'label'\n",
    "tamil_train['label'] = pd.Categorical(tamil_train['label'])  # Convert 'label' to categorical type\n",
    "\n",
    "tamil_dev = tamil_dev.iloc[:, 1:3]  # Select columns from index 1 to 2\n",
    "tamil_dev = tamil_dev.rename(columns={1: \"text\", 2: \"label\"})  # Rename columns to 'text' and 'label'\n",
    "tamil_dev['label'] = pd.Categorical(tamil_dev['label'])  # Convert 'label' to categorical type\n",
    "\n",
    "\n",
    "tamil_test = tamil_test.iloc[:, 1:2]  # Select columns from index 1 to 2\n",
    "tamil_test = tamil_test.rename(columns={1: \"text\"})  # Rename columns to 'text' and 'label'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.864428Z",
     "iopub.status.busy": "2025-03-30T05:32:11.864099Z",
     "iopub.status.idle": "2025-03-30T05:32:11.887433Z",
     "shell.execute_reply": "2025-03-30T05:32:11.886804Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.864399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1576, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.889886Z",
     "iopub.status.busy": "2025-03-30T05:32:11.889656Z",
     "iopub.status.idle": "2025-03-30T05:32:11.906159Z",
     "shell.execute_reply": "2025-03-30T05:32:11.905372Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.889868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Humanity paatha intha slag countries prblm var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanga Holi kondada oruku poranga da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avangala avanga ooruke anupura vazhiya parunga.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>பைத்தியமாடா நீ.. நான் சோழிய வேளாளர். நான் என் ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caste is there  parayan think to marry higheru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Humanity paatha intha slag countries prblm var...\n",
       "1               Avanga Holi kondada oruku poranga da\n",
       "2    Avangala avanga ooruke anupura vazhiya parunga.\n",
       "3  பைத்தியமாடா நீ.. நான் சோழிய வேளாளர். நான் என் ...\n",
       "4  Caste is there  parayan think to marry higheru..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lowercase & punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.908020Z",
     "iopub.status.busy": "2025-03-30T05:32:11.907832Z",
     "iopub.status.idle": "2025-03-30T05:32:11.923064Z",
     "shell.execute_reply": "2025-03-30T05:32:11.922502Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.908005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def lowercase_latin(text):\n",
    "#     return re.sub(r'[A-Z]', lambda x: x.group(0).lower(), text)\n",
    "\n",
    "# # Apply the function to the 'text' column\n",
    "# tamil_train['text'] = tamil_train['text'].apply(lowercase_latin)\n",
    "# tamil_dev['text'] = tamil_dev['text'].apply(lowercase_latin)\n",
    "# tamil_test['text'] = tamil_test['text'].apply(lowercase_latin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.923959Z",
     "iopub.status.busy": "2025-03-30T05:32:11.923741Z",
     "iopub.status.idle": "2025-03-30T05:32:11.938939Z",
     "shell.execute_reply": "2025-03-30T05:32:11.938371Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.923942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pandas as pd\n",
    "# import emoji\n",
    "\n",
    "# # Regular expression pattern to match emojis or emoticons\n",
    "# emoji_pattern = re.compile(\n",
    "#     \"[\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#     \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#     \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#     \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "#     \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "#     \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "#     \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "#     \"\\U0001FA00-\\U0001FA6F\"  # Chess symbols\n",
    "#     \"\\U0001FA70-\\U0001FAFF\"  # Symbols for Legacy Computing\n",
    "#     \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "#     \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "#     \"]+\"\n",
    "# )\n",
    "\n",
    "# # Function to extract emojis from the text\n",
    "# def extract_emojis(text):\n",
    "#     return emoji_pattern.findall(text)\n",
    "\n",
    "# # Function to replace emojis with their text meaning\n",
    "# def replace_emojis_with_meaning(text):\n",
    "#     return emoji.demojize(text, delimiters=(\" \", \" \"))  # Converts emojis to text with spaces\n",
    "\n",
    "# # Function to get the rows containing emojis and also return the emojis in those rows\n",
    "# def get_rows_with_emojis(text_series):\n",
    "#     rows_with_emojis = text_series[text_series.apply(lambda x: bool(emoji_pattern.search(x)))]\n",
    "#     emojis = rows_with_emojis.apply(extract_emojis)\n",
    "#     return rows_with_emojis, emojis\n",
    "\n",
    "# # train\n",
    "# emoji_rows, emojis = get_rows_with_emojis(tamil_train['text'])\n",
    "# print(f\"Number of rows containing emojis or emoticons: {len(emoji_rows)}\")\n",
    "# tamil_train.loc[emoji_rows.index, 'text'] = tamil_train.loc[emoji_rows.index, 'text'].apply(replace_emojis_with_meaning)\n",
    "\n",
    "# # dev\n",
    "# emoji_rows, emojis = get_rows_with_emojis(tamil_dev['text'])\n",
    "# print(f\"Number of rows containing emojis or emoticons: {len(emoji_rows)}\")\n",
    "# tamil_dev.loc[emoji_rows.index, 'text'] = tamil_dev.loc[emoji_rows.index, 'text'].apply(replace_emojis_with_meaning)\n",
    "\n",
    "# # for test\n",
    "# emoji_rows, emojis = get_rows_with_emojis(tamil_test['text'])\n",
    "# print(f\"Number of rows containing emojis or emoticons: {len(emoji_rows)}\")\n",
    "# tamil_test.loc[emoji_rows.index, 'text'] = tamil_test.loc[emoji_rows.index, 'text'].apply(replace_emojis_with_meaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.939984Z",
     "iopub.status.busy": "2025-03-30T05:32:11.939707Z",
     "iopub.status.idle": "2025-03-30T05:32:11.958026Z",
     "shell.execute_reply": "2025-03-30T05:32:11.957474Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.939959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek You (also a chat program)\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A** Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A**\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A** Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don’t care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can’t stop laughing\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.958816Z",
     "iopub.status.busy": "2025-03-30T05:32:11.958609Z",
     "iopub.status.idle": "2025-03-30T05:32:11.978829Z",
     "shell.execute_reply": "2025-03-30T05:32:11.978269Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.958799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:11.979988Z",
     "iopub.status.busy": "2025-03-30T05:32:11.979719Z",
     "iopub.status.idle": "2025-03-30T05:32:12.000559Z",
     "shell.execute_reply": "2025-03-30T05:32:11.999950Z",
     "shell.execute_reply.started": "2025-03-30T05:32:11.979962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def remove_punctuation(text):\n",
    "#     return re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "\n",
    "# tamil_train['text'] = tamil_train['text'].apply(remove_punctuation)\n",
    "# tamil_dev['text'] = tamil_dev['text'].apply(remove_punctuation)\n",
    "# tamil_test['text'] = tamil_test['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.001427Z",
     "iopub.status.busy": "2025-03-30T05:32:12.001208Z",
     "iopub.status.idle": "2025-03-30T05:32:12.022223Z",
     "shell.execute_reply": "2025-03-30T05:32:12.021583Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.001410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indha ariya kandupidippin moolam neenga solla ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@vijayakumarp7959  unmai therincha nee pesu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inga erukka yella dev... boys  vadakkan vadakk...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>பீகாரி பிரசாந்த் கிஷோரிடம் கொடுத்த 350 கோடியை ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mumbai Bangalore la 80% percentage outsiders</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Indha ariya kandupidippin moolam neenga solla ...     0\n",
       "1        @vijayakumarp7959  unmai therincha nee pesu     0\n",
       "2  Inga erukka yella dev... boys  vadakkan vadakk...     1\n",
       "3  பீகாரி பிரசாந்த் கிஷோரிடம் கொடுத்த 350 கோடியை ...     1\n",
       "4       Mumbai Bangalore la 80% percentage outsiders     1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.023116Z",
     "iopub.status.busy": "2025-03-30T05:32:12.022828Z",
     "iopub.status.idle": "2025-03-30T05:32:12.037022Z",
     "shell.execute_reply": "2025-03-30T05:32:12.036208Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.023088Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOSUR also mini North India bro ,,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@rajaa9979  வணிகர் சங்கங்களின் தலைமை தமிழரிடத...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Already Telangana becomes Hindi belt 🥺</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>வட மாநிலத்தொழிளார் தமிழ்நாட்டினரரை தாக்கினால் ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>தமிழன்  வட இந்தியாவிலும்  கேரளா  ஆந்திரா  கர்ந...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0                 HOSUR also mini North India bro ,,     0\n",
       "1   @rajaa9979  வணிகர் சங்கங்களின் தலைமை தமிழரிடத...     0\n",
       "2             Already Telangana becomes Hindi belt 🥺     0\n",
       "3  வட மாநிலத்தொழிளார் தமிழ்நாட்டினரரை தாக்கினால் ...     0\n",
       "4  தமிழன்  வட இந்தியாவிலும்  கேரளா  ஆந்திரா  கர்ந...     0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.038008Z",
     "iopub.status.busy": "2025-03-30T05:32:12.037824Z",
     "iopub.status.idle": "2025-03-30T05:32:12.168265Z",
     "shell.execute_reply": "2025-03-30T05:32:12.167532Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.037992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.169339Z",
     "iopub.status.busy": "2025-03-30T05:32:12.169073Z",
     "iopub.status.idle": "2025-03-30T05:32:12.307849Z",
     "shell.execute_reply": "2025-03-30T05:32:12.307031Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.169293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "device = 'cuda'\n",
    "device = device if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.308953Z",
     "iopub.status.busy": "2025-03-30T05:32:12.308647Z",
     "iopub.status.idle": "2025-03-30T05:32:12.315809Z",
     "shell.execute_reply": "2025-03-30T05:32:12.314961Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.308923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class tamil_Offensive_Dataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None, bpe=False):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.is_bpe_tokenized = bpe\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.is_bpe_tokenized:\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        else:\n",
    "            item = {\n",
    "                'input_ids': torch.LongTensor(self.encodings[idx].ids),\n",
    "                'attention_mask': torch.LongTensor(self.encodings[idx].attention_mask)\n",
    "            }\n",
    "        # Only add labels if available (for training/validation datasets)\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) if self.labels is not None else len(self.encodings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.316904Z",
     "iopub.status.busy": "2025-03-30T05:32:12.316626Z",
     "iopub.status.idle": "2025-03-30T05:32:12.332125Z",
     "shell.execute_reply": "2025-03-30T05:32:12.331204Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.316877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define models and configurations\n",
    "models = ['TamilBert']\n",
    "wbools = [True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.333267Z",
     "iopub.status.busy": "2025-03-30T05:32:12.332979Z",
     "iopub.status.idle": "2025-03-30T05:32:12.348725Z",
     "shell.execute_reply": "2025-03-30T05:32:12.348127Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.333231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_of_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T05:32:12.349842Z",
     "iopub.status.busy": "2025-03-30T05:32:12.349580Z",
     "iopub.status.idle": "2025-03-30T05:34:46.286969Z",
     "shell.execute_reply": "2025-03-30T05:34:46.285452Z",
     "shell.execute_reply.started": "2025-03-30T05:32:12.349812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d6212dd96b4b54b0b41e30d4a93343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d091f0c4e0fb451fb5f2cc5944b75e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e926931976c84f3286ce016c444b29ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77aaaf37539d46558eaaef04b6c5b5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee818b120b645f186f4e8a883a2d0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MURIL_base_cased_tamil_weighted\n",
      "Length of test_batch_sentences: 1576\n",
      "['Humanity paatha intha slag countries prblm varathu', 'Avanga Holi kondada oruku poranga da', 'Avangala avanga ooruke anupura vazhiya parunga.', 'பைத்தியமாடா நீ.. நான் சோழிய வேளாளர். நான் என் அப்பா தாத்தா வரை எல்லாம் OC  சாதி பிரிவில் இருந்தோம். .படிங்க டா. பிள்ளை பட்டம் போட்டவங்க துளுவ வேளாளர் எல்லாம் அகமுடையார் னு போட்டுகிட்டு  கள்ளர் மறவர் னு ஒன்று சேர்நதுகிட்டு சாதி பத்தி பேச வர்றீங்க. எந்த சாதி டா நீங்க', 'Caste is there  parayan think to marry higherup caste that ok their intention but no one acept it how their (sc),intentions supported by thiruttu karunanithi telugan mairu support for.first parayas must marry ponmudi daughter.daughter.son daughter or his relation moreover ponmudi must eat beef then he must give hiscadte lady to parayas to marry then this ponmudi paradesi tell or suppor sc ok']\n",
      "Length of test_encodings: 3\n",
      "Total test samples: 3\n",
      "Total test batches: 1\n",
      "==========================================================\n",
      "Epoch 0\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/345 [00:00<?, ?it/s]<ipython-input-18-360a0a568ef9>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "<ipython-input-18-360a0a568ef9>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "100%|██████████| 345/345 [01:54<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true shape: 787\n",
      "y_pred shape: 787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]<ipython-input-18-360a0a568ef9>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch 1: Processed 3 samples\n",
      "⚠️ Total processed test sentences: 3\n",
      "⚠️ Expected test sentences: 1576\n",
      "⚠️ Final length of test_preds: 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bf0989b22188>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_batch_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_test_predictions.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Track models and their predictions\n",
    "all_predictions = {}\n",
    "\n",
    "# Iterate over models\n",
    "for model, wbool in list(itertools.product(models, wbools)):\n",
    "    loss_weighted = wbool\n",
    "    # model_predictions = []\n",
    "    \n",
    "    # Model selection and initialization based on the model type\n",
    "    if model == 'MURIL':\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=2)  # Change to 2 for binary classification\n",
    "        model_name = 'MURIL_base_cased_tamil'\n",
    "    \n",
    "\n",
    "    if model == 'XLMR':\n",
    "        from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "        model_instance = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-large', num_labels=2)  # Change to 2 for binary classification\n",
    "        model_name = 'XLMroberta_large_tamil'\n",
    "    \n",
    "    \n",
    "    if model == 'mbertbase':\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-multilingual-cased')\n",
    "        model_instance = BertForSequenceClassification.from_pretrained('google-bert/bert-base-multilingual-cased', num_labels=2)  # Change to 2 for binary classification\n",
    "        model_name = 'Mbert_base_cased_tamil'\n",
    "    \n",
    "    if model == 'indic':\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/indic-bert\", num_labels=2)  # Change to 2 for binary classification\n",
    "        model_name = 'Indic_bert_tamil'\n",
    "\n",
    "    if model == 'TamilBert':\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"l3cube-pune/tamil-bert\")\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\"l3cube-pune/tamil-bert\", num_labels=2)  # Change to 2 for binary classification\n",
    "        model_name = 'TamilBert'\n",
    "\n",
    "    \n",
    "\n",
    "    if loss_weighted:\n",
    "        model_name = model_name + '_weighted'\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "    \n",
    "\n",
    "\n",
    "    from transformers import AdamW\n",
    "    optimizer = AdamW(model_instance.parameters(), lr=1e-5)\n",
    "\n",
    "    train_batch_sentences = list(tamil_train['text'])  # Extract the text data\n",
    "    train_batch_labels = list(tamil_train['label'])   # Extract the numeric labels (0 or 1)\n",
    "    dev_batch_sentences = list(tamil_dev['text'])      # Extract the text data\n",
    "    dev_batch_labels = list(tamil_dev['label'])\n",
    "    \n",
    "    test_batch_sentences = list(tamil_test['text'])      # Extract the text data\n",
    "    \n",
    "    print(f\"Length of test_batch_sentences: {len(test_batch_sentences)}\")\n",
    "    print(test_batch_sentences[:5])  # Print first 5 sentences\n",
    "\n",
    "    if 'parameters' in tokenizer.__dict__.keys() and tokenizer.__dict__['_parameters']['model'] == 'ByteLevelBPE':\n",
    "        train_encodings = tokenizer.encode_batch(train_batch_sentences)\n",
    "        dev_encodings = tokenizer.encode_batch(dev_batch_sentences)\n",
    "        # test_encodings = tokenizer.encode_batch(test_batch_sentences)\n",
    "    else:\n",
    "        train_encodings = tokenizer(train_batch_sentences, padding='max_length', truncation=True, max_length=115, return_tensors=\"pt\")\n",
    "        dev_encodings = tokenizer(dev_batch_sentences, padding='max_length', truncation=True, max_length=115, return_tensors=\"pt\")\n",
    "        test_encodings = tokenizer(\n",
    "            test_batch_sentences,  # Make sure this is a list of 1576 strings\n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=115, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    train_labels = torch.tensor(train_batch_labels)\n",
    "    dev_labels = torch.tensor(dev_batch_labels)\n",
    "\n",
    "    print(f\"Length of test_encodings: {len(test_encodings)}\")\n",
    "\n",
    "\n",
    "    # Defining Datasets\n",
    "    train_dataset = tamil_Offensive_Dataset(train_encodings, train_labels, bpe = False)\n",
    "    dev_dataset = tamil_Offensive_Dataset(dev_encodings, dev_labels, bpe = False)\n",
    "    test_dataset = tamil_Offensive_Dataset(test_encodings, bpe = False)\n",
    "    \n",
    "\n",
    "    # model_instance.to(device)\n",
    "    # model_instance.to(device)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model_instance.to(device)\n",
    "    best_val_f1 = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "    import torch.nn as nn\n",
    "    # weights = class_weight.compute_class_weight('balanced',np.unique(train_batch_labels),train_batch_labels)\n",
    "    # weights = class_weight.compute_class_weight('balanced', np.unique(train_batch_labels), train_batch_labels)\n",
    "    weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_batch_labels), y=train_batch_labels)\n",
    "\n",
    "    # weights = np.exp(weights)/np.sum(np.exp(weights))\n",
    "    class_weights = torch.FloatTensor(weights).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    print(f\"Total test samples: {len(test_dataset)}\")\n",
    "    print(f\"Total test batches: {len(test_loader)}\")\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        total_train_loss = 0\n",
    "        model_instance.train()\n",
    "        print(\"==========================================================\")\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        print(\"Train\")\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model_instance(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            if loss_weighted:\n",
    "                loss = loss_function(outputs.logits, labels)\n",
    "            else:\n",
    "                loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for logits in outputs[1].detach().cpu().numpy():\n",
    "                train_preds.append(np.argmax(logits))\n",
    "            for logits in labels.cpu().numpy():\n",
    "                train_labels.append(logits)\n",
    "            total_train_loss += loss.item()/len(train_loader)\n",
    "\n",
    "        print(\"Dev\")\n",
    "        dev_preds = []\n",
    "        model_instance.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch in tqdm(dev_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                # outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                outputs = model_instance(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "                if loss_weighted:\n",
    "                    loss = loss_function(outputs[1], labels)\n",
    "                else:\n",
    "                    loss = outputs[0]\n",
    "                total_val_loss += loss.item()/len(dev_loader)\n",
    "\n",
    "                for logits in outputs[1].cpu().numpy():\n",
    "                    dev_preds.append(np.argmax(logits))\n",
    "\n",
    "        y_true = dev_batch_labels\n",
    "        y_pred = dev_preds\n",
    "        print(f\"y_true shape: {len(y_true)}\")\n",
    "        print(f\"y_pred shape: {len(y_pred)}\")\n",
    "        target_names = ['Not caste/immigration hate speech', 'Caste/immigration hate speech']\n",
    "        train_report = classification_report(train_labels, train_preds, target_names=target_names)\n",
    "        report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "        val_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            test_preds = []  # Reset before loop\n",
    "            test_sentences_processed = 0  # Track processed sentences\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(tqdm(test_loader)):\n",
    "                    try:\n",
    "                        input_ids = batch['input_ids'].to(device)\n",
    "                        attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "                        outputs = model_instance(input_ids, attention_mask=attention_mask)\n",
    "                        batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            \n",
    "                        test_preds.extend(batch_preds)\n",
    "                        test_sentences_processed += len(batch_preds)\n",
    "            \n",
    "                        print(f\"✅ Batch {i+1}: Processed {len(batch_preds)} samples\")\n",
    "            \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error in batch {i+1}: {e}\")\n",
    "            \n",
    "            print(f\"⚠️ Total processed test sentences: {test_sentences_processed}\")\n",
    "            print(f\"⚠️ Expected test sentences: {len(test_batch_sentences)}\")\n",
    "            print(f\"⚠️ Final length of test_preds: {len(test_preds)}\")\n",
    "\n",
    "\n",
    "            test_df = pd.DataFrame({'text': test_batch_sentences, 'prediction': test_preds})\n",
    "            csv_path = os.path.join(model_save_path, model_name + \"_test_predictions.csv\")\n",
    "            test_df.to_csv(csv_path, index=False)\n",
    "\n",
    "            best_val_f1 = val_f1\n",
    "            count = 0\n",
    "\n",
    "            # Store best predictions\n",
    "            all_predictions[model_name] = y_pred.copy()\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "        print(train_report)\n",
    "        print(report)\n",
    "        print(\"Epoch {}, Train Loss = {}, Val Loss = {}, Val F1 = {}, Best Val f1 = {}, stagnant = {}\".format(epoch, total_train_loss, total_val_loss, val_f1, best_val_f1, count))\n",
    "        if count == 5:\n",
    "            print(\"No increase for 5 epochs, Stopping ...\")\n",
    "            break\n",
    "\n",
    "    import gc\n",
    "    del model_instance\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T05:34:46.287582Z",
     "iopub.status.idle": "2025-03-30T05:34:46.287843Z",
     "shell.execute_reply": "2025-03-30T05:34:46.287739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Define model path (same as where you saved it)\n",
    "model_path = \"/kaggle/working/finetuned_models/MURIL_base_cased_tamil_weighted\"\n",
    "\n",
    "# Load model + tokenizer directly\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T05:34:46.288945Z",
     "iopub.status.idle": "2025-03-30T05:34:46.289345Z",
     "shell.execute_reply": "2025-03-30T05:34:46.289155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def predict_labels(model, tokenizer, texts, batch_size=16):\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, max_length=115, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"])\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids, attention_mask = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # Get raw predictions\n",
    "            \n",
    "            batch_preds = torch.argmax(logits, dim=1).cpu().numpy()  # Convert to class labels\n",
    "            predictions.extend(batch_preds)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get texts from tamil_dev and tamil_test\n",
    "tamil_dev_texts = list(tamil_dev['text'])  # Dev dataset\n",
    "tamil_test_texts = list(tamil_test['text'])  # Test dataset\n",
    "\n",
    "# Predict\n",
    "dev_predictions = predict_labels(model, tokenizer, tamil_dev_texts)\n",
    "test_predictions = predict_labels(model, tokenizer, tamil_test_texts)\n",
    "\n",
    "# Print results\n",
    "print(f\"Dev Predictions: {dev_predictions[:10]}\")  # Print first 10 predictions\n",
    "print(f\"Test Predictions: {test_predictions[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result of every models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T05:34:46.290147Z",
     "iopub.status.idle": "2025-03-30T05:34:46.290538Z",
     "shell.execute_reply": "2025-03-30T05:34:46.290374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from tabulate import tabulate\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "for model, preds in all_predictions.items():\n",
    "    console.print(f\"\\n[bold cyan]🔹 Best Predictions for {model}:[/bold cyan] {preds[:10]}\", style=\"bold green\")  \n",
    "\n",
    "    # Compute metrics\n",
    "    model_f1 = f1_score(y_true, preds, average='macro')\n",
    "    model_accuracy = accuracy_score(y_true, preds)\n",
    "    model_report = classification_report(y_true, preds, target_names=target_names, digits=4, output_dict=True)\n",
    "\n",
    "    # Displaying results using Rich Table\n",
    "    table = Table(title=f\"[bold magenta]Performance Metrics for {model}[/bold magenta]\", show_header=True, header_style=\"bold yellow\")\n",
    "    table.add_column(\"Metric\", style=\"bold cyan\")\n",
    "    table.add_column(\"Score\", justify=\"center\", style=\"bold white\")\n",
    "    \n",
    "    table.add_row(\"Accuracy\", f\"{model_accuracy:.4f}\")\n",
    "    table.add_row(\"F1 Score (Macro)\", f\"{model_f1:.4f}\")\n",
    "\n",
    "    console.print(table)  # Print the table\n",
    "\n",
    "    # Displaying classification report in a tabular format\n",
    "    headers = [\"Class\", \"Precision\", \"Recall\", \"F1-score\", \"Support\"]\n",
    "    rows = []\n",
    "\n",
    "    for label, metrics in model_report.items():\n",
    "        if label in target_names:  # Avoid non-label keys\n",
    "            rows.append([label, f\"{metrics['precision']:.4f}\", f\"{metrics['recall']:.4f}\", f\"{metrics['f1-score']:.4f}\", int(metrics['support'])])\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))  # Fancy grid format\n",
    "    print(\"=\" * 100)  # Separator for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority voting and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-30T05:34:46.291172Z",
     "iopub.status.idle": "2025-03-30T05:34:46.291557Z",
     "shell.execute_reply": "2025-03-30T05:34:46.291397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert predictions to a NumPy array\n",
    "predictions_matrix = np.array(list(all_predictions.values()))  # Shape: (num_models, num_samples)\n",
    "\n",
    "# Apply majority voting\n",
    "ensemble_predictions, _ = mode(predictions_matrix, axis=0)\n",
    "\n",
    "# Convert the result to a 1D array\n",
    "ensemble_predictions = ensemble_predictions.flatten()\n",
    "\n",
    "# Compute evaluation metrics\n",
    "ensemble_f1 = f1_score(y_true, ensemble_predictions, average='macro')  # Macro F1-score\n",
    "ensemble_accuracy = accuracy_score(y_true, ensemble_predictions)  # Accuracy score\n",
    "ensemble_report = classification_report(y_true, ensemble_predictions, target_names=target_names)\n",
    "\n",
    "# Print results\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Ensemble F1 Score: {ensemble_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", ensemble_report)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6986888,
     "sourceId": 11192006,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
